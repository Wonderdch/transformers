- ä»»åŠ¡è¯´æ˜ï¼šå¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå¹¶è¿›è¡Œè¯­ä¹‰æ£€ç´¢
- ä»»åŠ¡è¦æ±‚ï¼š
    - åŠ è½½æ–‡æœ¬ç¼–ç æ¨¡å‹
    - å¯¹æé—®å’Œæ–‡æ¡£è¿›è¡Œç¼–ç ï¼Œå¹¶è¿›è¡Œæ£€ç´¢
- æ‰“å¡è¦æ±‚ï¼šåŠ è½½ä¸‰ä¸ªç¼–ç æ¨¡å‹ï¼Œè®¡ç®—æ£€ç´¢ç»“æœ
## è¯­ä¹‰æ£€ç´¢æµç¨‹

è¯­ä¹‰æ£€ç´¢æ˜¯é€šè¿‡è¯åµŒå…¥å’Œå¥å­åµŒå…¥ç­‰æŠ€æœ¯ï¼Œå°†æ–‡æœ¬è¡¨ç¤ºä¸ºè¯­ä¹‰ä¸°å¯Œçš„å‘é‡ã€‚é€šè¿‡ç›¸ä¼¼åº¦è®¡ç®—å’Œç»“æœæ’åºæ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚ç”¨æˆ·æŸ¥è¯¢ç»è¿‡è‡ªç„¶è¯­è¨€å¤„ç†å¤„ç†ï¼Œæœ€ç»ˆç³»ç»Ÿè¿”å›ç»è¿‡æ’åºçš„ç›¸å…³æ–‡æ¡£ï¼Œæä¾›ç”¨æˆ·å‹å¥½çš„ä¿¡æ¯å±•ç¤ºã€‚è¯­ä¹‰æ£€ç´¢é€šè¿‡æ·±åº¦å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·æŸ¥è¯¢ï¼Œæé«˜æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆæœã€‚

```mermaid
graph TD
    A[åŠ è½½æ¨¡å‹] -->|Sentence Transformer| B((ç¼–ç æ–‡æœ¬))
    B -->|é—®é¢˜å¥å­| C[é—®é¢˜Embeddings]
    B -->|PDFå†…å®¹å¥å­| D[PDFå†…å®¹Embeddings]
    C -->|æ ‡å‡†åŒ–| C
    D -->|æ ‡å‡†åŒ–| D
    C -->|ç›¸ä¼¼åº¦è®¡ç®—| E[ç›¸ä¼¼åº¦çŸ©é˜µ]
    D -->|ç›¸ä¼¼åº¦è®¡ç®—| E
    E -->|æ’åº| F[æ’åºåçš„ç›¸ä¼¼åº¦]
    F -->|é€‰å–æœ€å¤§å€¼| G[æœ€ç›¸ä¼¼çš„é¡µç ]
    G -->|å†™å…¥ç»“æœ| H[ç”Ÿæˆæäº¤ç»“æœ]
```

## æ–‡æœ¬ç¼–ç æ¨¡å‹

æ–‡æœ¬ç¼–ç æ¨¡å‹å¯¹äºè¯­ä¹‰æ£€ç´¢çš„ç²¾åº¦è‡³å…³é‡è¦ã€‚ç›®å‰ï¼Œå¤§å¤šæ•°è¯­ä¹‰æ£€ç´¢ç³»ç»Ÿé‡‡ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ–‡æœ¬ç¼–ç ï¼Œå…¶ä¸­æœ€ä¸ºå¸¸è§çš„æ˜¯åŸºäº BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰çš„æ¨¡å‹ï¼Œæˆ–è€…ä½¿ç”¨ GPTï¼ˆGenerative Pre-trained Transformerï¼‰ç­‰ã€‚è¿™äº›é¢„è®­ç»ƒæ¨¡å‹é€šè¿‡åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæ•æ‰è¯è¯­å’Œå¥å­ä¹‹é—´çš„å¤æ‚è¯­ä¹‰å…³ç³»ã€‚é€‰æ‹©åˆé€‚çš„æ–‡æœ¬ç¼–ç æ¨¡å‹ç›´æ¥å½±å“åˆ°å¾—åˆ°çš„æ–‡æœ¬å‘é‡çš„æœ‰æ•ˆæ€§ï¼Œè¿›è€Œå½±å“æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆæœã€‚

- ç¼–ç æ¨¡å‹æ’è¡Œæ¦œï¼š[MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)
    - å¯ä»¥å•ç‹¬æŸ¥çœ‹ è‹±æ–‡ ä¸­æ–‡ çš„æ’è¡Œæ¦œ

- ä¸‹é¢å‡ ä¸ªæ–‡æœ¬ç¼–ç æ¨¡å‹éƒ½ç”¨åˆ°äº† [SentenceTransformers](https://www.sbert.net/)ï¼Œå› ä¸ºå€ŸåŠ©å®ƒå¯ä»¥ç”¨æ›´å°‘çš„ä»£ç å®ç°æ–‡æœ¬æˆ–å›¾åƒçš„åµŒå…¥
    - ä¸ä¾èµ–å®ƒä¹Ÿèƒ½å†™ï¼Œä½†éœ€è¦è‡ªå·±å†™ Transformer å’Œ PyTorch ä»£ç 

- M3E
    - [moka-ai/m3e-small Â· Hugging Face](https://huggingface.co/moka-ai/m3e-small)
    - m3e-base çš„ embedding æ˜¯ (768,)ï¼›m3e-small çš„ embedding æ˜¯ (512,)
```python
!pip install sentence-transformers

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('moka-ai/m3e-small')

question_sentences = [x['question'] for x in questions]
pdf_content_sentences = [x['content'] for x in pdf_content]

question_embeddings = model.encode(question_sentences, normalize_embeddings=True)
pdf_embeddings = model.encode(pdf_content_sentences, normalize_embeddings=True)

for query_idx, feat in enumerate(question_embeddings):
    score = feat @ pdf_embeddings.T
    max_score_page_idx = score.argsort()[-1] + 1
    questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx)

with open('submit.json', 'w', encoding='utf8') as up:
    json.dump(questions, up, ensure_ascii=False, indent=4)
```

- BGE
    - [BAAI/bge-small-zh-v1.5 Â· Hugging Face](https://huggingface.co/BAAI/bge-small-zh-v1.5)
```python
import json
import jieba
import pdfplumber
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from sentence_transformers import SentenceTransformer

# è¯»å–æ•°æ®é›†
questions = json.load(open("questions.json"))
pdf = pdfplumber.open("åˆèµ›è®­ç»ƒæ•°æ®é›†.pdf")
pdf_content = []

def split_text_fixed_size(text, chunk_size):
  return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)] 

for page_idx in range(len(pdf.pages)):
  text = pdf.pages[page_idx].extract_text()
  for chunk_text in split_text_fixed_size(text, 40):
    pdf_content.append({
      'page':'page_'+str(page_idx + 1),
      'content':chunk_text
    })

model = SentenceTransformer('BAAI/bge-small-zh-v1.5')

question_sentences = [x['question'] for x in questions]
pdf_content_sentences = [x['content'] for x in pdf_content]

question_embeddings = model.encode(question_sentences, normalize_embeddings=True, show_progress_bar=True)
pdf_embeddings = model.encode(pdf_content_sentences, normalize_embeddings=True, show_progress_bar=True)

for query_idx, feat in enumerate(question_embeddings):
    score = feat @ pdf_embeddings.T
    max_score_page_idx = score.argsort()[-1]
    questions[query_idx]['reference'] = pdf_content[max_score_page_idx]['page']

with open('wonder_bge_sgement_retrieval_top1.json', 'w', encoding='utf8') as up:
    json.dump(questions, up, ensure_ascii=False, indent=4)


def remove_duplicates(input_list):
  seen = set()
  result = []
  for item in input_list:
    if item not in seen:
      seen.add(item)
      result.append(item)
  return result

for query_idx,feat in enumerate(question_embeddings):
  score = feat @ pdf_embeddings.T
  max_score_page_idx = score.argsort()[::-1]
  pages =[pdf_content[x]['page']for x in max_score_page_idx]
  questions[query_idx]['reference'] = remove_duplicates(pages[:100])[:10]

with open('wonder_bge_sgement_retrieval_top10.json', 'w', encoding='utf8') as up:
    json.dump(questions, up, ensure_ascii=False, indent=4)
```

- BCEmbedding
```python
model = SentenceTransformer("/maidalun1020/bce-embedding-base_v1", device='cuda') model.max_seq_length = 512

# å‰©ä½™ä»£ç ä¸M3Eéƒ¨åˆ†ç›¸åŒ
```
## æ–‡æœ¬åˆ‡åˆ†æ–¹æ³•

æ–‡æœ¬çš„é•¿åº¦æ˜¯å¦ä¸€ä¸ªå…³é”®å› ç´ ï¼Œå½±å“äº†æ–‡æœ¬ç¼–ç çš„ç»“æœã€‚çŸ­æ–‡æœ¬å’Œé•¿æ–‡æœ¬åœ¨ç¼–ç æˆå‘é‡æ—¶å¯èƒ½è¡¨è¾¾ä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ã€‚å³ä½¿ä¸¤è€…åŒ…å«ç›¸åŒçš„å•è¯æˆ–æœ‰ç›¸ä¼¼çš„è¯­ä¹‰ï¼Œç”±äºä¸Šä¸‹æ–‡çš„ä¸åŒï¼Œå¾—åˆ°çš„å‘é‡ä¹Ÿä¼šæœ‰æ‰€ä¸åŒã€‚å› æ­¤ï¼Œå½“åœ¨è¯­ä¹‰æ£€ç´¢ä¸­ä½¿ç”¨çŸ­æ–‡æœ¬æ¥æ£€ç´¢é•¿æ–‡æœ¬æ—¶ï¼Œæˆ–è€…åä¹‹ï¼Œå¯èƒ½å¯¼è‡´ä¸€å®šçš„è¯¯å·®ã€‚é’ˆå¯¹æ–‡æœ¬é•¿åº¦çš„å·®å¼‚ï¼Œæœ‰äº›ç³»ç»Ÿé‡‡ç”¨æˆªæ–­æˆ–å¡«å……ç­‰æ–¹å¼å¤„ç†ï¼Œä»¥ä¿æŒä¸€è‡´çš„å‘é‡è¡¨ç¤ºã€‚

æ›´å¤šé˜…è¯»èµ„æ–™ï¼š
- [Text Splitters | ğŸ¦œï¸ğŸ”— Langchain](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
- [åˆ†å‰²æ¡ˆä¾‹ ChunkViz (railway.app)](https://chunkviz.up.railway.app/)

|åç§°|åˆ†å‰²ä¾æ®|æè¿°|
|---|---|---|
|é€’å½’å¼åˆ†å‰²å™¨|ä¸€ç»„ç”¨æˆ·å®šä¹‰çš„å­—ç¬¦|é€’å½’åœ°åˆ†å‰²æ–‡æœ¬ã€‚é€’å½’åˆ†å‰²æ–‡æœ¬çš„ç›®çš„æ˜¯å°½é‡ä¿æŒç›¸å…³çš„æ–‡æœ¬æ®µè½ç›¸é‚»ã€‚è¿™æ˜¯å¼€å§‹æ–‡æœ¬åˆ†å‰²çš„æ¨èæ–¹å¼ã€‚|
|HTML åˆ†å‰²å™¨|HTML ç‰¹å®šå­—ç¬¦|åŸºäº HTML ç‰¹å®šå­—ç¬¦è¿›è¡Œæ–‡æœ¬åˆ†å‰²ã€‚ç‰¹åˆ«åœ°ï¼Œå®ƒä¼šæ·»åŠ æœ‰å…³æ¯ä¸ªæ–‡æœ¬å—æ¥æºçš„ç›¸å…³ä¿¡æ¯ï¼ˆåŸºäº HTML ç»“æ„ï¼‰ã€‚|
|Markdown åˆ†å‰²å™¨|Markdown ç‰¹å®šå­—ç¬¦|åŸºäº Markdown ç‰¹å®šå­—ç¬¦è¿›è¡Œæ–‡æœ¬åˆ†å‰²ã€‚ç‰¹åˆ«åœ°ï¼Œå®ƒä¼šæ·»åŠ æœ‰å…³æ¯ä¸ªæ–‡æœ¬å—æ¥æºçš„ç›¸å…³ä¿¡æ¯ï¼ˆåŸºäº Markdown ç»“æ„ï¼‰ã€‚|
|ä»£ç åˆ†å‰²å™¨|ä»£ç ï¼ˆPythonã€JSï¼‰ç‰¹å®šå­—ç¬¦|åŸºäºç‰¹å®šäºç¼–ç è¯­è¨€çš„å­—ç¬¦è¿›è¡Œæ–‡æœ¬åˆ†å‰²ã€‚æ”¯æŒä» 15 ç§ä¸åŒçš„ç¼–ç¨‹è¯­è¨€ä¸­é€‰æ‹©ã€‚|
|Token åˆ†å‰²å™¨|Tokens|åŸºäº Token è¿›è¡Œæ–‡æœ¬åˆ†å‰²ã€‚å­˜åœ¨ä¸€äº›ä¸åŒçš„ Token è®¡é‡æ–¹æ³•ã€‚|
|å­—ç¬¦åˆ†å‰²å™¨|ç”¨æˆ·å®šä¹‰çš„å­—ç¬¦|åŸºäºç”¨æˆ·å®šä¹‰çš„å­—ç¬¦è¿›è¡Œæ–‡æœ¬åˆ†å‰²ã€‚è¿™æ˜¯è¾ƒä¸ºç®€å•çš„åˆ†å‰²æ–¹æ³•ä¹‹ä¸€ã€‚|
|è¯­ä¹‰åˆ†å—å™¨|å¥å­|é¦–å…ˆåŸºäºå¥å­è¿›è¡Œåˆ†å‰²ã€‚ç„¶åï¼Œå¦‚æœå®ƒä»¬åœ¨è¯­ä¹‰ä¸Šè¶³å¤Ÿç›¸ä¼¼ï¼Œå°±å°†ç›¸é‚»çš„å¥å­ç»„åˆåœ¨ä¸€èµ·ã€‚|

å¯¹äºè‡ªç„¶è¯­è¨€ï¼Œæ¨èä½¿ç”¨ Token åˆ†å‰²å™¨ï¼Œç»“åˆ Chunk Size å’Œ Overlap Size å¯ä»¥å¾—åˆ°ä¸åŒçš„åˆ‡åˆ†ï¼š
- **Chunk Sizeï¼ˆå—å¤§å°ï¼‰**ï¼šè¡¨ç¤ºå°†æ–‡æœ¬åˆ’åˆ†ä¸ºè¾ƒå°å—çš„å¤§å°ã€‚è¿™æ˜¯åˆ†å‰²åæ¯ä¸ªç‹¬ç«‹æ–‡æœ¬å—çš„é•¿åº¦æˆ–å®¹é‡ã€‚å—å¤§å°çš„é€‰æ‹©å–å†³äºåº”ç”¨çš„éœ€æ±‚å’Œå¯¹æ–‡æœ¬ç»“æ„çš„ç†è§£ã€‚
    - Chunck Size çš„å¤§å°è®¾ç½®å¯ä»¥å‚è€ƒç”¨æˆ·æé—®çš„ token å¤§å°ï¼Œæœ€ç®€å•å°±æ˜¯å°† size å¤§å°è®¾ç½®ä¸ºç”¨æˆ·æé—®å¹³å‡ token å¤§å°
- **Overlap Sizeï¼ˆé‡å å¤§å°ï¼‰**ï¼šæŒ‡ç›¸é‚»ä¸¤ä¸ªæ–‡æœ¬å—ä¹‹é—´çš„é‡å éƒ¨åˆ†çš„å¤§å°ã€‚åœ¨åˆ‡å‰²æ–‡æœ¬æ—¶ï¼Œé€šå¸¸å¸Œæœ›ä¿ç•™ä¸€äº›ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé‡å å¤§å°å°±æ˜¯æ§åˆ¶è¿™ç§ä¸Šä¸‹æ–‡ä¿ç•™çš„å‚æ•°ã€‚
    - è®¾å®š overlap åå¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šé¿å…åˆ†å‰²å¯¼è‡´çš„ä¿¡æ¯ä¸¢å¤±

```python
import json
import jieba
import pdfplumber
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from sentence_transformers import SentenceTransformer

# è¯»å–æ•°æ®é›†
questions = json.load(open("questions.json"))
pdf = pdfplumber.open("åˆèµ›è®­ç»ƒæ•°æ®é›†.pdf")
pdf_content = []

def split_text_fixed_size(text, chunk_size):
  return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)] 

for page_idx in range(len(pdf.pages)):
  text = pdf.pages[page_idx].extract_text()
  for chunk_text in split_text_fixed_size(text, 40):
    pdf_content.append({
      'page':'page_'+str(page_idx + 1),
      'content':chunk_text
    })

print(pdf_content[2])
print(pdf_content[3])

model = SentenceTransformer('moka-ai/m3e-small')

question_sentences = [x['question'] for x in questions]
pdf_content_sentences = [x['content'] for x in pdf_content]

question_embeddings = model.encode(question_sentences, normalize_embeddings=True)
pdf_embeddings = model.encode(pdf_content_sentences, normalize_embeddings=True)

for query_idx, feat in enumerate(question_embeddings):
    score = feat @ pdf_embeddings.T
    max_score_page_idx = score.argsort()[-1] + 1
    questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx)

with open('submit_m3e_small_sgement.json', 'w', encoding='utf8') as up:
    json.dump(questions, up, ensure_ascii=False, indent=4)
```