#todo 

- [动手学 RAG](https://coggle.club/blog/30days-of-ml-202401)
- [RAG.ipynb - Colaboratory (google.com)](https://colab.research.google.com/drive/1btol7iKYKNwmQU_8JatmyRK-NoLGIGDu#scrollTo=PkXHis01kYkj)
- 官方代码参考：[[RAG打卡.ipynb]]
## Part1 内容介绍

在自然语言处理领域，大型语言模型（LLM）如 GPT-3、BERT 等已经取得了显著的进展，它们能够生成连贯、自然的文本，回答问题，并执行其他复杂的语言任务。然而，这些模型存在一些固有的局限性，如“模型幻觉问题”、“时效性问题”和“数据安全问题”。为了克服这些限制，检索增强生成（Retrieval Augmented Generation，RA）技术应运而生。

[![](Attachment/89ea67452aa439c9bafcf192afc26624d82bd50a_2_690x387.jpeg.jpg)


RAG 技术结合了大型语言模型的强大生成能力和检索系统的精确性。它允许模型在生成文本时，从外部知识库中检索相关信息，从而提高生成内容的准确性、相关性和时效性。这种方法不仅增强了模型的回答能力，还减少了生成错误信息的风险。

本月的学习内容主要围绕检索增强生成（RAG）技术展开：
- RAG 技术背景与动机
- RAG 技术基本原理和技术流程
- 知识库构建与管理、检索模块技术
- ChatGPT/ChatGLM 的 API 使用
## Part2 活动安排

- 免费学习活动，不会收取任何费用。
- **请各位同学添加下面微信，并回复【竞赛学习】，即可参与。**

[![|291](Attachment/291.jpg)
## Part4 动手学 RAG
### 背景介绍

本次活动参赛选手以大模型为中心制作一个问答系统，回答用户的汽车相关问题。参赛选手需要根据问题，在文档中定位相关信息的位置，并根据文档内容通过大模型生成相应的答案。涉及的问题主要围绕汽车使用、维修、保养等方面。

```
问题1：怎么打开危险警告灯？ 答案1：危险警告灯开关在方向盘下方，按下开关即可打开危险警告灯。
问题2：车辆如何保养？ 答案2：为了保持车辆处于最佳状态，建议您定期关注车辆状态，包括定期保养、洗车、内部清洁、外部清洁、轮胎的保养、低压蓄电池的保养等。
问题3：靠背太热怎么办？ 答案3：您好，如果您的座椅靠背太热，可以尝试关闭座椅加热功能。在多媒体显示屏上依次点击空调开启按键→座椅→加热，在该界面下可以关闭座椅加热。
```
### 打卡任务

|任务名称|所需技能|
|---|---|
|任务 1：初始 RAG|无|
|任务 2：ChatGPT/GLM API 使用|Python|
|任务 3：读取汽车问答数据|Python|
|任务 4：文本索引与答案检索|TFIDF、BM25|
|任务 5：文本嵌入与向量检索|Embedding、transformer|
|任务 6：文本多路召回与重排序|ReRank|
|任务 7：文本问答 Promopt 优化|Python|
|任务 8：问答意图识别（进阶方向）|BERT/TFIDF|
|任务 9：问答关键词提取（进阶方向）|TextRank|
|任务 10：扩展词与扩展查询（进阶方向）|Word2Vec/BART|
|任务 11：本地微调 ChatGLM（进阶方向）|ChatGLM|
## 任务 1：初识 RAG

- 任务说明：了解到现有大模型的缺陷和 RAG 的优点和流程
- 任务要求：
    - 了解大模型现有的缺点
    - 理解 RAG 的流程和实现步骤
    - 清楚 RAG 的需要的技术
- [x] 打卡要求：阅读 LangChain 与 RAG 的[文章](https://juejin.cn/post/7310569308916334604)和 LangChain 的[官方文档](https://python.langchain.com/docs/expression_language/cookbook/retrieval)，列举 LangChain 能实现的功能。
    - [ ] [Langchain 开发](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzg2ODA5NTM1OA==&action=getalbum&album_id=3115055723522015235#wechat_redirect)
### 大模型的局限性

大型语言模型在自然语言处理领域展示了显著的能力，但它们也存在一系列固有的缺点。首先，虽然这些模型在掌握大量信息方面非常有效，但它们的结构和参数数量使得对其进行修改、微调或重新训练变得异常困难，且相关成本相当可观。

其次，大型语言模型的应用往往依赖于构建适当的提示（prompt）来引导模型生成所需的文本。这种方法通过将信息嵌入到提示中，从而引导模型按照特定的方向生成文本。然而，这种基于提示的方法可能使模型过于依赖先前见过的模式，而无法真正理解问题的本质。

|**大模型现存问题**|大型语言模型的局限性|
|---|---|
|问题 1.1|模型幻觉问题：生成内容可能不准确或不一致|
|问题 1.2|时效性问题：生成的内容不具有当前时效性|
|问题 1.3|数据安全问题：可能存在敏感信息泄露风险|

在自然语言处理领域，幻觉（Hallucination）被定义为生成的内容与提供的源内容无关或不忠实，具体而言，是一种虚假的感知，但在表面上却似乎是真实的。在一般语境中，幻觉是一个心理学术语，指的是一种特定类型的感知。在自然语言处理或大型语言模型的语境下，这种感知即为一种虚假的、不符合实际的信息。

造成幻觉的原因主要可以归结为数据驱动原因、表示和解码的不完善以及参数知识偏见。首先，数据对不齐或不匹配可能导致幻觉，因为模型在训练中未能准确地理解源内容与参考内容之间的关系。
### 知识库问答（Knowledge Base Question Answering，KBQA）

知识库问答（Knowledge Base Question Answering，简称 KBQA）是一种早期的对话系统方法，旨在利用结构化的知识库进行自然语言问题的回答。这种方法基于一个存储在图数据库中的知识库，通常以三元组的形式表示为<主题，关系，对象>，其中每个三元组都附带相关的属性信息。

[![](Attachment/6d92f9b53fba0f7abf262f63d3eeb9f90b276c53.png)

知识库问答早期是对话系统中的有效方法，其基于知识图谱的结构为系统提供了丰富的语义信息，使得系统能够更深入地理解用户提出的问题，并以结构化的形式回答这些问题。随着技术的不断发展，KBQA 方法也在不断演进，为对话系统的进一步提升奠定了基础。

在 KBQA 中，有两种主流方法用于处理自然语言问题：

- 主题识别与实体链接：该方法从识别问题中的主题开始，将其链接到知识库中的实体（称为主题实体）。通过主题实体，系统能够在知识库中查找相关的信息并回答问题。
- 多跳查询：基于图数据库的优势，KBQA 能够进行多跳查询，即通过多个关系跨越多个实体来获取更深层次的信息。这种灵活性使得系统能够更全面地理解和回答用户的复杂问题。
### RAG 介绍

检索增强生成（RAG）技术在弥补大型语言模型（LLM）的局限性方面取得了显著进展，尤其是在解决幻觉问题和提升实效性方面。在之前提到的 LLM 存在的问题中，特别是幻觉问题和时效性问题，RAG 技术通过引入外部知识库的检索机制，成功提升了生成内容的准确性、相关性和时效性。

[![](Attachment/4d9a56f001d4bbf70f8b1e17ed7ce9db1daebe78_2_690x371.webp)

- RAG 技术通过检索外部知识库，避免了幻觉问题的困扰。相较于单纯依赖大型语言模型对海量文本数据的学习，RAG 允许模型在生成文本时从事实丰富的外部知识库中检索相关信息。
- RAG 技术的时效性优势使其在处理实效性较强的问题时更为可靠。通过与外部知识库的连接，RAG 确保了模型可以获取最新的信息，及时适应当前的事件和知识。
- 与传统的知识库问答（KBQA）相比，RAG 技术在知识检索方面更加灵活，不仅能够从结构化的知识库中检索信息，还能够应对非结构化的自然语言文本。

|**RAG 优点**|描述|
|---|---|
|优点 1.1|提高准确性和相关性|
|优点 1.2|改善时效性，使模型适应当前事件和知识|
|优点 1.3|降低生成错误风险，依赖检索系统提供的准确信息|

**RAG 被构建为一个应用于大型语言模型的框架，其目标是通过结合大模型的生成能力和外部知识库的检索机制，提升自然语言处理任务的效果。** RAG 并非旨在取代已有的知识库问答（KBQA）系统，而是作为一种补充，利用检索机制强调实时性和准确性，从而弥补大型语言模型固有的局限性。

[![|700](Attachment/700.jpg)

RAG 框架的最终输出被设计为一种协同工作模式，将检索到的知识融合到大型语言模型的生成过程中。在应对任务特定问题时，RAG 会生成一段标准化的句子，引导大模型进行回答。下面是 RAG 输出到大型语言模型的典型模板：

```
你是一个{task}方面的专家，请结合给定的资料，并回答最终的问题。请如实回答，如果问题在资料中找不到答案，请回答不知道。

问题：{question}

资料：
- {information1}
- {information2}
- {information3}
```

其中，`{task}` 代表任务的领域或主题，`{question}` 是最终要回答的问题，而 `{information1}`、`{information2}` 等则是提供给模型的外部知识库中的具体信息。
### RAG 和 SFT 对比

在更新大型语言模型的知识方面，微调模型和使用 RAG 这两种方法有着各自的优缺点。微调模型优势在于能够通过有监督学习的方式，通过对任务相关数据的反复迭代调整，使得模型更好地适应特定领域的知识和要求。RAG 能够从外部知识库中检索最新、准确的信息，从而提高了答案的质量和时效性。其优势在于可以利用最新的外部信息，从而更好地适应当前事件和知识。

| |微调模型|RAG|
|---|---|---|
|优点|针对特定任务调整预训练模型。优点是可针对特定任务优化；|结合检索系统和生成模型。优点是能利用最新信息，提高答案质量，具有更好的可解释性和适应性：|
|缺点|但缺点是更新成本高，对新信息适应性较差；|是可能面临检索质量问题和曾加额外计算资源需求;|

|特性|RAG 技术|SFT 模型微调|
|---|---|---|
|知识更新|实时更新检索库，适合动态数据，无需频繁重训|存储静态信息，更新知识需要重新训练|
|外部知识|高效利用外部资源，适合各类数据库|可对齐外部知识，但对动态数据源不够灵活|
|数据处理|数据处理需求低|需构建高质量数据集，数据限制可能影响性能|
|模型定制化|专注于信息检索和整合，定制化程度低|可定制行为，风格及领域知识|
|可解释性|答案可追溯，解释性高|解释性相对低|
|计算资源|需要支持检索的计算资源，维护外部数据源|需要训练数据集和微调资源|
|延迟要求|数据检索可能增加延迟|微调后的模型反应更快|
|减少幻觉|基于实际数据，幻觉减少|通过特定域训练可减少幻觉，但仍然有限|
|道德和隐私|处理外部文本数据时需要考虑隐私和道德问题|训练数据的敏感内容可能引发隐私问题|
### RAG 实现流程

如果使用 RAG，主要包括信息检索和大型语言模型调用两个关键过程。信息检索通过连接外部知识库，获取与问题相关的信息；而大型语言模型调用则用于将这些信息整合到自然语言生成的过程中，以生成最终的回答。

|**RAG 流程**|描述|
|---|---|
|步骤 1：问题理解|准确把握用户的意图|
|步骤 2：知识检索|从知识库中相关的知识检索|
|步骤 3：答案生成|将检索结果与问题|

RAG 每个步骤都面临一些挑战，这些挑战使得 RAG 的实现变得复杂而困难。在问题理解阶段，系统需要准确把握用户的意图。**用户提问往往是短文本，而知识库中的信息可能是长文本。** 将用户提问与知识库中的知识建立有效的关联是一个难点，特别是考虑到用户提问可能模糊，用词不规范，难以直接找到相关的知识。

知识检索是 RAG 流程中的关键步骤，但也是面临挑战的步骤之一。**用户提问可能以多种方式表达，而知识库的信息来源可能是多样的，包括 PDF、PPT、Neo4j 等格式。**

**此外用户的意图可能非常灵活，可能是提问，也可能需要进行闲聊** 。在这个阶段，需要确保生成的答案与用户的意图一致，同时保持自然、连贯的文本。此外，大型模型的输出可能存在幻觉问题，即生成的内容可能与问题不相关，增加了生成准确回答的难度。

在论文综述 [「Retrieval-Augmented Generation for Large Language Models: A Survey」](https://arxiv.org/pdf/2312.10997.pdf) 中，作者将 RAG 技术按照复杂度继续划分为 Naive RAG，Advanced RAG、Modular RAG。

| **技术类型**         | **描述**                                                                                                                                                                                             |
| ---------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Naive RAG**    | Naive RAG 是 RAG 技术的最基本形式，也被称为经典 RAG。包括索引、检索、生成三个基本步骤。索引阶段将文档库分割成短的 Chunk，并构建向量索引。检索阶段根据问题和 Chunks 的相似度检索相关文档片段。生成阶段以检索到的上下文为条件，生成问题的回答。                                                            |
| **Advanced RAG** | Advanced RAG 在 Naive RAG 的基础上进行优化和增强。包含额外处理步骤，分别在数据索引、检索前和检索后进行。包括更精细的数据清洗、设计文档结构和添加元数据，以提升文本一致性、准确性和检索效率。在检索前使用问题的重写、路由和扩充等方式对齐问题和文档块之间的语义差异。在检索后通过重排序避免“Lost in the Middle”现象，或通过上下文筛选与压缩缩短窗口长度。 |
| **Modular RAG**  | Modular RAG 引入更多具体功能模块，例如查询搜索引擎、融合多个回答等。技术上融合了检索与微调、强化学习等。流程上对 RAG 模块进行设计和编排，出现多种不同 RAG 模式。提供更大灵活性，系统可以根据应用需求选择合适的功能模块组合。模块化 RAG 的引入使得系统更自由、灵活，适应不同场景和需求。                                          |

![[Pasted image 20240329195110.png|1000]]

在 RAG 技术流程中，涉及多个关键模块，每个模块承担着特定的任务，协同工作以实现准确的知识检索和生成自然语言回答。

| **技术模块**   | **描述**                                                                        |
| ---------- | ----------------------------------------------------------------------------- |
| **意图理解**   | 意图理解模块负责准确把握用户提出的问题，确定用户的意图和主题。处理用户提问的模糊性和不规范性，为后续流程提供清晰的任务目标。                |
| **文档解析**   | 文档解析模块用于处理来自不同来源的文档，包括 PDF、PPT、Neo4j 等格式。该模块负责将文档内容转化为可处理的结构化形式，为知识检索提供合适的输入。 |
| **文档索引**   | 文档索引模块将解析后的文档分割成短的 Chunk，并构建向量索引。或通过全文索引进行文本检索，使得系统能够更快速地找到与用户问题相关的文档片段。      |
| **向量嵌入**   | 向量嵌入模块负责将文档索引中的内容映射为向量表示，以便后续的相似度计算。这有助于模型更好地理解文档之间的关系，提高知识检索的准确性。            |
| **知识检索**   | 知识检索模块根据用户提问和向量嵌入计算的相似度检索或文本检索打分。这一步骤需要解决问题和文档之间的语义关联，确保检索的准确性。               |
| **重排序**    | 重排序模块在知识检索后对文档库进行重排序，以避免“Lost in the Middle”现象，确保最相关的文档片段在前面。                 |
| **大模型回答**  | 大模型回答模块利用大型语言模型生成最终的回答。该模块结合检索到的上下文，以生成连贯、准确的文本回答。                            |
| **其他功能模块** | 可根据具体应用需求引入其他功能模块，如查询搜索引擎、融合多个回答等。模块化设计使得系统更加灵活，能够根据不同场景选择合适的功能模块组合。          |
